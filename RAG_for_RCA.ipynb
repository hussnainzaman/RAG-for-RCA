{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbljYHMQcBh4"
      },
      "source": [
        "  Initalize Libraries and mount drive for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZCzY3wFS5u9G"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain==0.1\n",
        "!pip install -U langchain_openai\n",
        "!pip install -U openai\n",
        "!pip install -U ragas\n",
        "!pip install -U arxiv\n",
        "!pip install -U pymupdf\n",
        "!pip install -U tiktoken\n",
        "!pip install -U accelerate\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U datasets\n",
        "!pip install -U sentence_transformers\n",
        "!pip install -U FlagEmbedding\n",
        "!pip install -U ninja\n",
        "!pip install -U flash_attn --no-build-isolation\n",
        "!pip install -U tqdm\n",
        "!pip install -U rank_bm25\n",
        "!pip install -U transformers\n",
        "!pip install langchain pinecone-client\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -U torch\n",
        "!pip install openai\n",
        "!pip install gradio_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6ro4w-CWtmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4901dd48-13dc-4d22-e58c-63dd8555e79d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLtHG2maRTAg"
      },
      "outputs": [],
      "source": [
        "testcase = 'High_Test'\n",
        "issue=0\n",
        "# Case = 'Without_RAG'\n",
        "# Case = 'Synthetic_RAG_Strict_Prompt'\n",
        "# Case = 'Synthetic_RAG_Focused_Prompt'\n",
        "# Case = 'Synthetic_RAG_Extensive_Prompt'\n",
        "# Case = 'Training_RAG_Strict_Prompt'\n",
        "# Case = 'Trainig_RAG_Foucsed_Prompt'\n",
        "# Case = 'Training_RAG_Extensive_Prompt'\n",
        "\n",
        "json_path = '/content/drive/MyDrive/RCA/path_sets.json'\n",
        "path_setsp = '/content/drive/MyDrive/RCA-of-Cloud-Microservices-main/path_sets.json'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# _______________For Synthetic DataSets_______________\n",
        "\n",
        "# model_e = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# api_key = '6c58af3e-b5bf-4bb0-9598-c6939b4690b0'\n",
        "# pc = Pinecone(api_key=api_key)\n",
        "# dimension = model_e.get_sentence_embedding_dimension()\n",
        "# index_name = \"comsynth\"\n",
        "\n",
        "# if index_name not in pc.list_indexes().names():\n",
        "#     pc.create_index(\n",
        "#         name=index_name,\n",
        "#         dimension=dimension,\n",
        "#         metric=\"cosine\",\n",
        "#         spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "#     )\n",
        "\n",
        "# index = pc.Index(name=index_name)\n",
        "\n",
        "# _______________For Training DataSets_______________\n",
        "\n",
        "# # Initialize the model and tokenizer\n",
        "model_name = \"BAAI/bge-large-en-v1.5\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "model.to('cpu')\n",
        "\n",
        "# Initialize Pinecone\n",
        "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "pc = Pinecone(api_key='6c58af3e-b5bf-4bb0-9598-c6939b4690b0')\n",
        "# ____________Change index name when changing scanrio(low to high, temp 1, temp2.) use db names\n",
        "index_name = \"lowtrain\"\n",
        "dimension = model.config.hidden_size\n",
        "\n",
        "# Create or connect to Pinecone index\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric='cosine',\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "    )\n",
        "index = pc.Index(name=index_name)"
      ],
      "metadata": {
        "id": "ydlpaeyRMk-v",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "faf5b12dbb5f439eb17c08c05dde1dd1",
            "f490a25d9c9845379ab661ebc3188c01",
            "bd8213dc373f4cda8a7533fd619728f6",
            "eb072299c00f47f9b1e9b2b72994ce94",
            "ee67bb3f13db457eb3010c7cacff0b3f",
            "301e2528247e4f82b38ad166c2fb424a",
            "b68e18ab4a8f4416936bd9f42e7348c1",
            "1e58a6c0fab349678dcc8fbe8ab40e65",
            "10aed61ee49a4344882845f89614573f",
            "74b4a49613f04546b7ab898c3be09131",
            "f123efea6e664245a589a05c61bb28c5",
            "d1609db3b0e24b368ec026342436f3b2",
            "df9a9a14c2bf4b508c62698e1221e28b",
            "c3710d2c055c47d2bbde81ec0e1fd428",
            "8fea67e9c20b498da386db257975ce7f",
            "91c40ecaf8ee4263bd5a956f2d44ffca",
            "0cea35f2221648a5959355851ecd43a8",
            "a23e1a4a66504fc99d16dfa66cc86d42",
            "e1fb7ff0cd464212907ebda34b22bed9",
            "cd9f02224bc04fedb3be03b9988177c5",
            "bae43eabd30d402eb1d5782059bae459",
            "5c9915743ef949069a29bb101e9c5a32",
            "4979407108be4c069701572a1b650175",
            "80d1bbda3fdb4bf494ae5541f5838c8b",
            "2046aa6f9aa24978a2d2c67c70f267dc",
            "96b829b57af0410a8c73d106535476ca",
            "7d04a1858e9342ef96042f323708ccae",
            "3a8ad6b25e364a6bbb2b375413986fb9",
            "3e7da2089e824c47bebad18c385272b6",
            "7af8fc633c7341cabc5dbfcdc9f169bd",
            "8e4294223f7442849c54d1b84b6c5385",
            "2d1f9a5ba09f43c682ad09b2ec479faa",
            "94e0ec413d2845cc92567bbda3d7e877",
            "6a0364e5fa7040af96a6c7e6c3d6fd0b",
            "393ace59578c4a23bcd18fac0dc20b67",
            "84571623e02f48cba2c32cf13b7c2f3e",
            "820cf438e2534cb1a210c553e7f2a4f8",
            "bfb41e046a2f49ad9db84e9b6ebe24c3",
            "83f84436f55b4599890b4b913273e2f5",
            "803b3c8f7aa54f17be2dd4359f865462",
            "238b07734d5046e5a9ac66fd57632dd8",
            "e3170e94dbf84b929208d8241a7d6856",
            "774c09b3ff0c4fa48da168327848094e",
            "da78b41e88334cbe9aa49cb1c36d5e39",
            "2ea84cff156a472e990844f6c77cc5d8",
            "6d3db8f6f81741f09eb90db30a79d531",
            "60f16ffa1f8740cbbb9c5f1b77004933",
            "0a55e33d6f3b44dfa755b0ec7449dced",
            "6694eb9a868249dd867fe80c62b248eb",
            "d42aad45aa7b4865ad1ff79ee38bef09",
            "36760fb3c1594a3ab1e29b45a1817e3b",
            "18c48924849e4d2e8f4f1536b7d03a71",
            "61d7a308248749f0996f4027f2e37a42",
            "56718a46578540d69ba6e113d55acbd3",
            "2e12f942b5ea4498a0ea4ef7f76e867a",
            "298c74d429084086bb41bfa10915f4f7",
            "7f4fe3a8e809469a8923299602d894ed",
            "e8efa9c183784075a885d3b870709a5a",
            "04a6d077863b40188d36e6adb301aef1",
            "2bb69366e9164e2088e25f01a6c37def",
            "ba447f835ad3419cb814f85fdacb0889",
            "ae5e4c4b657746f987f908342e0e34ee",
            "26c4b0050e70497789a6518821d325db",
            "9a09054bfdee45599778958ed5fd6dba",
            "90febd569aac481795bb38f53b199531",
            "6638a7fc4be74410b0a202cf4413bfff"
          ]
        },
        "outputId": "2dd9639e-8b52-4e51-e5c0-e34d89d029e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "faf5b12dbb5f439eb17c08c05dde1dd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1609db3b0e24b368ec026342436f3b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4979407108be4c069701572a1b650175"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a0364e5fa7040af96a6c7e6c3d6fd0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ea84cff156a472e990844f6c77cc5d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "298c74d429084086bb41bfa10915f4f7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from scipy.stats import median_abs_deviation\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from gradio_client import Client\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from openai import Client, OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "api_key = userdata.get('RAG')\n",
        "\n",
        "# Function to check if a service exists in the loaded service information\n",
        "def service_exists(service_name):\n",
        "    exists = service_name in service_info\n",
        "    # print(f\"Service '{service_name}' exists: {exists}\")\n",
        "    return exists\n",
        "\n",
        "\n",
        "def get_embeddings(text):\n",
        "    \"\"\"Generate embeddings for input text\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move tensor to CPU before converting to numpy\n",
        "    return embeddings\n",
        "\n",
        "# Function to calculate Median Absolute Deviation (MAD) scores for given metrics\n",
        "def calculate_mad_scores(dataframe):\n",
        "    # print(\"Calculating MAD scores...\")\n",
        "    metrics = [\n",
        "        \"availability_Average\",\n",
        "        \"latency_Average\",\n",
        "        \"latency_p50\",\n",
        "        \"latency_p90\",\n",
        "        \"latency_p95\",\n",
        "        \"latency_p99\",\n",
        "        \"requests_Sum\",\n",
        "    ]\n",
        "    mad_columns = [m + \"_MAD\" for m in metrics]\n",
        "    for m in metrics:\n",
        "        dataframe.loc[:, m + \"_MAD\"] = dataframe.groupby(\"microservice\")[m].transform(\n",
        "            lambda x: median_abs_deviation(x, scale=\"normal\")\n",
        "        )\n",
        "    dataframe[\"Max_MAD_Score\"] = dataframe[mad_columns].max(axis=1)\n",
        "    dataframe[\"Metric_With_Max_MAD\"] = (\n",
        "        dataframe[mad_columns].idxmax(axis=1).str.replace(\"_MAD\", \"\")\n",
        "    )\n",
        "    # print(\"MAD scores calculated successfully.\")\n",
        "    return dataframe.loc[dataframe.groupby(\"microservice\")[\"Max_MAD_Score\"].idxmax()]\n",
        "\n",
        "# __________________________START OF PROMPT TEMPLATES_______________________________________________________\n",
        "\n",
        "\n",
        "                                                                    #  Without RAG\n",
        "# ___________________________________________________________________________________________________________\n",
        "\n",
        "# def generate_analysis_prompt(\n",
        "#     service_name,\n",
        "#     mad_score,\n",
        "#     affected_metric,\n",
        "#     dependencies,\n",
        "#     dependents,\n",
        "#     retrieval_results,\n",
        "#     log,\n",
        "# ):\n",
        "#     dependencies_formatted = \", \".join(dependencies) if dependencies else \"None\"\n",
        "#     dependents_formatted = \", \".join(dependents) if dependents else \"None\"\n",
        "#\n",
        "\n",
        "#     prompt = f\"\"\"\n",
        "#     .\n",
        "\n",
        "#     Context:\n",
        "#     An anomaly with a Median Absolute Deviation (MAD) score of '{mad_score}' has been detected in the '{service_name}' service’s '{affected_metric}' metric. This service is a critical component of a pet adoption website’s microservices architecture\n",
        "#     Your analysis should focus on identifying a singular root cause from\n",
        "#     among the dependencies and dependents. Consider each dependency's role\n",
        "#     and potential issues that could lead to such a deviation.\n",
        "#     Additionally, pinpoint the primary dependent (target node) that is\n",
        "#     most directly affected by this anomaly. This should be the service that\n",
        "#     relies on '{service_name}' and would face the most significant impact due to\n",
        "#     the anomaly in '{affected_metric}'. If no target node found from the data,\n",
        "#     declare the service itself as the target node. Use the provided data to support each analytical step. Ensure that the analysis is logical and coherent, focusing on causality and impact within the microservices environment\n",
        "\n",
        "#     Dependencies and Dependents Information:\n",
        "\n",
        "#     The service relies on the following direct dependencies:\n",
        "#     '{dependencies_formatted}'.\n",
        "#     The service also serves as a crucial dependency for:\n",
        "#     '{dependents_formatted}' Services.\n",
        "\n",
        "#     Current Service Log for anaomaly detected in {service_name}:\n",
        "#     '{log}'\n",
        "\n",
        "#     Historcal Anomalies MetaData:\n",
        "\n",
        "#     No Historical data found you should use only avaiable data to support you analysis.\n",
        "\n",
        "#     Objectives:\n",
        "\n",
        "#     1. Identify the 'singular root cause' node among the dependencies and dependents.\n",
        "#     2. Identify the 'primary target node' among the dependents directly impacted by\n",
        "#        this anomaly.\n",
        "\n",
        "#     Outcome or Output Results:\n",
        "#     Provide detailed analysis identifying on the following:\n",
        "\n",
        "#     1. Singular Root Cause Node.\n",
        "#     2. Primary Target Node.\n",
        "#     3. Your deatiled analysis hypothesis should guide subsequent investigation and mitigation efforts\n",
        "#     Instructions:\n",
        "#     - Structure the response to ensure a logical flow, with each section addressing specific aspects as detailed above..\"\"\"\n",
        "#     print(prompt)\n",
        "\n",
        "#     return prompt\n",
        "\n",
        "\n",
        "# __________________________________________________________________________________________________________\n",
        "\n",
        "                                                                    #  Strict\n",
        "# ___________________________________________________________________________________________________________\n",
        "\n",
        "# def generate_analysis_prompt(\n",
        "#     service_name,\n",
        "#     mad_score,\n",
        "#     affected_metric,\n",
        "#     dependencies,\n",
        "#     dependents,\n",
        "#     retrieval_results,\n",
        "#     log,\n",
        "# ):\n",
        "#     dependencies_formatted = \", \".join(dependencies) if dependencies else \"None\"\n",
        "#     dependents_formatted = \", \".join(dependents) if dependents else \"None\"\n",
        "#     historical_anomalies = retrieval_results\n",
        "\n",
        "#     prompt = f\"\"\"\n",
        "#\n",
        "\n",
        "#     Context:\n",
        "#     An anomaly with a Median Absolute Deviation (MAD) score of '{mad_score}' has been detected in the '{service_name}' service’s '{affected_metric}' metric. This service is a critical component of a pet adoption website’s microservices architecture.\n",
        "#     Your analysis should focus on identifying a singular root cause from\n",
        "#     among the dependencies and dependents. Consider each dependency's role\n",
        "#     and potential issues that could lead to such a deviation.\n",
        "#     Additionally, pinpoint the primary dependent (target node) that is\n",
        "#     most directly affected by this anomaly. This should be the service that\n",
        "#     relies on '{service_name}' and would face the most significant impact due to\n",
        "#     the anomaly in '{affected_metric}'. If no target node found from the data,\n",
        "#     declare the service itself as the target node.\n",
        "\n",
        "#     Dependencies and Dependents Information:\n",
        "\n",
        "#     The service relies on the following direct dependencies:\n",
        "#     '{dependencies_formatted}'.\n",
        "#     The service also serves as a crucial dependency for:\n",
        "#     '{dependents_formatted}' Services.\n",
        "\n",
        "#     Current Service Log for anaomaly detected in {service_name}:\n",
        "#     '{log}'\n",
        "\n",
        "#\n",
        "\n",
        "#     Historcal Anomalies MetaData:\n",
        "\n",
        "#     '{historical_anomalies}' You can use this data which is a historical data to support your analysis.\n",
        "#     In historical data there is node path information and you cal also use other parameters like p99, p50, etc.\n",
        "\n",
        "#     Objectives:\n",
        "\n",
        "#     1. Identify the 'singular root cause' node among the dependencies and dependents.\n",
        "#     2. Identify the 'primary target node' among the dependents directly impacted by\n",
        "#        this anomaly.\n",
        "\n",
        "#     Outcome or Output Results:\n",
        "#     Provide detailed analysis identifying on the following:\n",
        "\n",
        "#     1. Singular Root Cause Node.\n",
        "#     2. Primary Target Node.\n",
        "#     Instructions:\n",
        "#     - Structure the response to ensure a logical flow, with each section addressing specific aspects as detailed above.\n",
        "#     \"\"\"\n",
        "#     print(prompt)\n",
        "\n",
        "#     return prompt\n",
        "\n",
        "\n",
        "# __________________________________________________________________________________________________________\n",
        "\n",
        "                                                                    #  FOCUSED\n",
        "# ___________________________________________________________________________________________________________\n",
        "# def generate_analysis_prompt(service_name, mad_score, affected_metric, dependencies, dependents, retrieval_results):\n",
        "#   dependencies_formatted = ', '.join(dependencies) if dependencies else 'None'\n",
        "#   dependents_formatted = ', '.join(dependents) if dependents else 'None'\n",
        "#   historical_anomalies = retrieval_results\n",
        "\n",
        "#   prompt = f\"\"\"\n",
        "\n",
        "#   Context:\n",
        "\n",
        "#   An anomaly with a Median Absolute Deviation (MAD) score of '{mad_score}' has been detected in the '{service_name}' service’s '{affected_metric}' metric. This service is a critical component of a pet adoption website’s microservices architecture..\n",
        "#   Your analysis should be on identifying a singular root cause from\n",
        "#   among the dependencies and dependents. Consider each dependency's role\n",
        "#   and potential issues that could lead to such a deviation.\n",
        "#   Additionally, pinpoint the primary dependent (target node) that is\n",
        "#   most directly affected by this anomaly. This should be the service that\n",
        "#   relies on '{service_name}' and would face the most significant impact due to\n",
        "#   the anomaly in '{affected_metric}'. If no target node found from the data,\n",
        "#   declare the service itself as the target node.\n",
        "#   Use the provided data to support each analytical step. Ensure that the analysis is logical and coherent, focusing on causality and impact within the microservices environment\n",
        "\n",
        "#   Dependencies and Dependents Information:\n",
        "\n",
        "#   The service relies on the following dependencies = '\"{dependencies_formatted}.\"'\n",
        "#   The service also serves as a crucial dependency for '\"{dependents_formatted}.\"'\n",
        "\n",
        "#   Current Service Log for anaomaly detected in {service_name}:\n",
        "#   '{log}'\n",
        "\n",
        "#   Historcal Anomalies MetaData:\n",
        "\n",
        "#   '{historical_anomalies}'\n",
        "#   You can use this data which is a historical data to support your analysis. In historical data there is node path information which you can use to support your analysis.\n",
        "\n",
        "#   Objectives:\n",
        "\n",
        "#   1. Identify the 'singular root cause' node among the dependencies and dependents.\n",
        "#       -Evaluate the interactions within the service dependency chain.\n",
        "#       -Identify the likely starting point of the anomalies based on the roles and interactions detailed in the logs and summary.\n",
        "#       -Analyze how the identified root cause node's functionality could lead to observed issues.\n",
        "#   2. Identify the 'primary target node' among the dependents directly impacted by\n",
        "#      this anomaly.\n",
        "#   3. Assess the impact on dependent services and discuss the operational and integration challenges faced by affected nodes within the overall architecture.\n",
        "\n",
        "#   Outcome or Conclusion Results:\n",
        "#     - Extract Name for: ''Singular Root Cause Node'' and ''Primary Target Node''\n",
        "#     - Summarize the identified root cause and most affected target node.\n",
        "#     - Discuss how the dependencies and dependents contribute to the propagation of issues.\n",
        "#  Instructions:\n",
        "#     - Structure the response to ensure a logical flow, with each section addressing specific aspects as detailed above.\n",
        "# .\"\"\"\n",
        "#   print(prompt)\n",
        "\n",
        "#   return prompt\n",
        "\n",
        "# __________________________________________________________________________________________________________\n",
        "\n",
        "                                                                    # Extensive\n",
        "# ___________________________________________________________________________________________________________\n",
        "# def generate_analysis_prompt(service_name, mad_score, affected_metric, dependencies, dependents, retrieval_results,log):\n",
        "#   dependencies_formatted = ', '.join(dependencies) if dependencies else 'None'\n",
        "#   dependents_formatted = ', '.join(dependents) if dependents else 'None'\n",
        "#   historical_anomalies = retrieval_results\n",
        "\n",
        "#   prompt = f\"\"\"\n",
        "\n",
        "#   Context:\n",
        "\n",
        "#   An anomaly with a Median Absolute Deviation (MAD) score of '{mad_score}' has been detected in the '{service_name}' service’s '{affected_metric}' metric. This service is a critical component of a pet adoption website’s microservices architecture..\n",
        "#   Your analysis should be on identifying a singular root cause from\n",
        "#   among the dependencies and dependents. Consider each dependency's role\n",
        "#   and potential issues that could lead to such a deviation.\n",
        "#   Additionally, pinpoint the primary dependent (target node) that is\n",
        "#   most directly affected by this anomaly. This should be the service that\n",
        "#   relies on '{service_name}' and would face the most significant impact due to\n",
        "#   the anomaly in '{affected_metric}'. If no target node found from the data,\n",
        "#   declare the service itself as the target node.\n",
        "#   Use the provided data to support each analytical step. Ensure that the analysis is logical and coherent, focusing on causality and impact within the microservices environment\n",
        "\n",
        "#   Dependencies and Dependents Information:\n",
        "\n",
        "#   The service relies on the following dependencies = '\"{dependencies_formatted}.\"'\n",
        "#   The service also serves as a crucial dependency for '\"{dependents_formatted}.\"'\n",
        "\n",
        "\n",
        "#   Current Service Log for anaomaly detected in {service_name}:\n",
        "#   '{log}'\n",
        "\n",
        "#   Historcal anomaly MetaData:\n",
        "\n",
        "#   '{historical_anomalies}'\n",
        "#   Use this data which is a historical data to support your analysis. In historical data there is node path information which you can use to support your analysis.\n",
        "\n",
        "#   Objectives:\n",
        "\n",
        "#   1. Identify the 'singular root cause' node and 'Primary Target Node'.\n",
        "#   2. Dependencies and Their Impact:\n",
        "#     - Analyze the influence of 'PetSearch_AWS::ECS::Fargate' on its direct dependencies.\n",
        "#     - Assess how issues originating from 'PetSearch_AWS::ECS::Fargate' propagate to dependent services, affecting system performance and reliability.\n",
        "#   3. Pathways of Impact:\n",
        "#     - Map out the key pathways through which the issues are transmitted within the system.\n",
        "#   4. Metrics and Effects:\n",
        "#     - Evaluate how the issues affect critical performance metrics like latency and availability.\n",
        "#   5. Mitigation Strategies:\n",
        "#     - Propose actionable mitigation strategies to address the current issues.\n",
        "#     - Suggest preventive measures to enhance system resilience against similar future anomalies.\n",
        "\n",
        "#   Outcome or Conclusion Results:\n",
        "#     - Extract Name for: ''Singular Root Cause Node'' and ''Primary Target Node''\n",
        "#     - Provide detailed insights into dependency-related impacts and propagation mechanisms.\n",
        "#     - Offer specific recommendations for both immediate resolution and long-term preventive strategies.\n",
        "#     - Discuss how the dependencies and dependents contribute to the propagation of issues.\n",
        "\n",
        "#  Instructions:\n",
        "#     - Structure the response to ensure a logical flow, with each section addressing specific aspects as detailed above.\n",
        "#     - Highlight the importance of data-driven decision-making in managing microservice architectures.\n",
        "\n",
        "# .\"\"\"\n",
        "#   print(prompt)\n",
        "\n",
        "#   return prompt\n",
        "\n",
        "# __________________________END OF PROMPT TEMPLATES_________________________________________________________________\n",
        "\n",
        "# Function to analyze root cause\n",
        "def get_response(prompt):\n",
        "    \"\"\"Get response from the model.\"\"\"\n",
        "\n",
        "    system_context = \"\"\"As an Anomaly Detection Engineer at an Amazon data center, your task is to detect anomalies using current and historical data, including historical anomalies metadata with path information of dependencies and dependents.\n",
        "    Your objective is to identify the 'Root Cause Node' by evaluating each dependency's role and potential issues contributing to the anomaly, and analyzing provided metrics for insights.\n",
        "    Additionally, you need to pinpoint the 'Primary Target Node' by determining the most affected dependent service or declaring {service_name} itself if no significant impact is found.\n",
        "    The outcome of your analysis should focus on identifying the singular root cause node causing the anomaly in the service and the primary target node most impacted by the anomaly, or confirming service_name if it is the root cause.\n",
        "    Utilize historical data to support your analysis and enhance your findings, guiding subsequent investigation and mitigation efforts with a focused and comprehensive identification of the root cause and primary target node of the anomalies.\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "    data = {\n",
        "    \"model\": \"gpt-3.5-turbo-1106\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_context\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"top_p\": 0.9,\n",
        "    \"frequency_penalty\": 0.7,\n",
        "    \"presence_penalty\": 0.0\n",
        "}\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    response_json = response.json()\n",
        "    # print(response_json)\n",
        "\n",
        "    # response = input(\"paste here\")\n",
        "    # response =  pipe(prompt, max_new_tokens=1200, num_return_sequences=1, temperature=0.8, top_p=0.9)[0]['generated_text']\n",
        "    return response_json\n",
        "\n",
        "def analyze_root_cause(anomaly_row, data):\n",
        "    \"\"\"Analyze root cause.\"\"\"\n",
        "    service_name = anomaly_row[\"microservice\"]\n",
        "    log = anomaly_row\n",
        "    mad = anomaly_row[\"Max_MAD_Score\"]\n",
        "    print(f\"Analyzing root cause for service: {service_name}\")\n",
        "    if not service_exists(service_name):\n",
        "        return \"Service information not found.\", \"\", []\n",
        "    df = data\n",
        "    # print(df)\n",
        "    prompt = generate_analysis_prompt(\n",
        "        service_name,\n",
        "        anomaly_row[\"Max_MAD_Score\"],\n",
        "        anomaly_row[\"Metric_With_Max_MAD\"],\n",
        "        service_info[service_name][\"dependencies\"],\n",
        "        service_info[service_name][\"dependents\"],\n",
        "        df,\n",
        "        log,\n",
        "    )\n",
        "\n",
        "    print(f\"Retrieving and generating response based on the prompt...\")\n",
        "    # client = Client(\"KingNish/Real-Time-Chat-with-AI\")\n",
        "    # response = client.predict(\n",
        "    #     text=prompt, model=\"Nous Hermes Mixtral 8x7B DPO\", api_name=\"/predict\"\n",
        "    # )\n",
        "    response = get_response(prompt)\n",
        "    text_content = response['choices'][0]['message']['content']\n",
        "    print(f\"Response generated: {text_content}\")\n",
        "    return prompt, text_content\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        metrics_data_filtered = calculate_mad_scores(metrics_data)\n",
        "        print(\"Detected Top Service\\n\")\n",
        "        top_services = metrics_data_filtered.sort_values(\n",
        "            by=\"Max_MAD_Score\", ascending=False\n",
        "        ).head(1)\n",
        "        # print(top_services)\n",
        "        results = []\n",
        "        print(top_services)\n",
        "        row_dict = top_services.iloc[0]  # Select the first row as a Series\n",
        "        # Format the row into a string\n",
        "        row_text = \" \".join([f\"{key}: {value}\" for key, value in row_dict.items()])\n",
        "        # print(row_text)\n",
        "# _______________For Training DataSets_______________\n",
        "        embedding = get_embeddings(row_text)\n",
        "# _______________For Synthetic DataSets______________\n",
        "        # embedding = model_e.encode(row_text)  # Use for Synthtic\n",
        "\n",
        "\n",
        "        # print(embedding)\n",
        "        vector_id = f\"{testcase}_{issue}\"\n",
        "        vector = {\n",
        "            \"id\": vector_id,\n",
        "            \"values\": embedding.tolist(),\n",
        "            \"metadata\": row_dict,  # Store the row as a dictionary in metadata\n",
        "        }\n",
        "\n",
        "        # print(\"Generating Query Vector\")\n",
        "        # print(f\"Vector ID: {vector['id']}\")\n",
        "        # # print(f\"Vector Values: {vector['values']}\")\n",
        "        # print(\n",
        "        #     f\"Max_MAD_Score: {vector['metadata'].get('Max_MAD_Score')}, Microservice: {vector['metadata'].get('microservice')}, Metric_With_Max_MAD: {vector['metadata'].get('Metric_With_Max_MAD')}, Requests_Sum: {vector['metadata'].get('requests_Sum')}\\n\"\n",
        "        # )\n",
        "        # print(\"###Query Vector Generated###\")\n",
        "        # metadata_filter = {\"microservice\": vector['metadata'].get('microservice')}  # Filtering to match entries with the same microservice\n",
        "        query_embedding = (embedding.tolist())  # Use the embedding from the max MAD score row in test db\n",
        "        # print(f\"Query Embedding: {query_embedding}\")\n",
        "        query_results = index.query(\n",
        "            vector=query_embedding, top_k=1, include_metadata=True\n",
        "        )\n",
        "        # print(f\"Query Results: {query_results}\")\n",
        "        # print(\"Query Results:\")\n",
        "        # Print the results\n",
        "        for match in query_results[\"matches\"]:\n",
        "            metadata = match[\"metadata\"]\n",
        "            # print(f\"Match ID: {match['id']}\")\n",
        "            # print(f\"Score: {match['score']}\")\n",
        "            # print(f\"Values: {match['values']}\")\n",
        "            # print(f\"Metadata: {metadata}\\n\")\n",
        "            # print(f\"Metadata: Max_MAD_Score: {metadata.get('Max_MAD_Score')}, Microservice: {metadata.get('microservice')}, Metric_With_Max_MAD: {metadata.get('Metric_With_Max_MAD')}, Requests_Sum: {metadata.get('requests_Sum')}\\n\")\n",
        "\n",
        "        for _, row in top_services.iterrows():\n",
        "            print(f\"Analyzing root cause for service: {row['microservice']}\")\n",
        "            if service_exists(row[\"microservice\"]):\n",
        "                prompt, response = analyze_root_cause(row, metadata)\n",
        "                results.append(\n",
        "                    {\n",
        "                        \"TestCase\": testcase,\n",
        "                        \"Scenario\":Case,\n",
        "                        \"Issue_ID\": issue,\n",
        "                        \"MicroService\": row[\"microservice\"],\n",
        "                        \"Timestamp\": row[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                        \"MAD Score\": row[\"Max_MAD_Score\"],\n",
        "                        \"Affected Metric\": row[\"Metric_With_Max_MAD\"],\n",
        "                        \"Prompt\": prompt,\n",
        "                        \"Response\": response,\n",
        "                        \"Root Cause Node\" : \"\",\n",
        "                        \"Target Node\" : \"\",\n",
        "                    }\n",
        "                )\n",
        "                print(f\"Analysis complete for service: {row['microservice']}\")\n",
        "            else:\n",
        "                print(\n",
        "                    f\"Service {row['microservice']} not found in service information.\"\n",
        "                )\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        # print(results_df)\n",
        "\n",
        "        # Path to save results\n",
        "        results_file_path = f\"/content/drive/MyDrive/Yan Liu Project Documents/All_Results_RAW/LLM_{Case}_{testcase}.csv\"\n",
        "        # Append results to CSV file\n",
        "        if not os.path.isfile(results_file_path):\n",
        "            results_df.to_csv(results_file_path, mode=\"a\", header=True, index=False)\n",
        "        else:\n",
        "            results_df.to_csv(results_file_path, mode=\"a\", header=False, index=False)\n",
        "\n",
        "        print(\"Analysis results saved.\")\n",
        "        time.sleep(5)\n",
        "    except KeyError as ke:\n",
        "        print(f\"KeyError occurred: {ke}\")\n",
        "    except IndexError as ie:\n",
        "        print(f\"IndexError occurred: {ie}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    finally:\n",
        "        print(\"End of main function.\")\n",
        "\n",
        "# issue = 1\n",
        "\n",
        "# Loop through a range, but you're only executing it once due to `range(1)`\n",
        "for issue in range(0,18):\n",
        "    print(f'Executing Issue# {issue}')\n",
        "    file_path_test = f\"/content/drive/MyDrive/DATA/Transformed_metric_low_test/transformed_transposed_issue{issue}_metrics.csv\"\n",
        "    json_path = \"/content/drive/MyDrive/RCA/path_sets.json\"\n",
        "    # Load dataset from a specified CSV file\n",
        "    # print(f\"Loading dataset from '{file_path_test}'.\")\n",
        "    metrics_data = pd.read_csv(file_path_test)\n",
        "    metrics_data[\"timestamp\"] = pd.to_datetime(\n",
        "        metrics_data[\"timestamp\"]\n",
        "    )  # Convert 'timestamp' column to datetime objects\n",
        "    # print(\"Dataset loaded successfully.\")\n",
        "\n",
        "    # Load JSON data containing service information\n",
        "    # print(f\"Loading JSON data from '{json_path}'...\")\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    # print(\"JSON data loaded successfully.\")\n",
        "\n",
        "    # Process the JSON data to build a dictionary mapping services to their dependencies and dependents\n",
        "    service_info = {}\n",
        "    for path in data:\n",
        "        if len(path) > 1:\n",
        "            for i in range(len(path) - 1):\n",
        "                service = path[i]\n",
        "                dependency = path[i + 1]\n",
        "                if service not in service_info:\n",
        "                    service_info[service] = {\"dependencies\": [], \"dependents\": []}\n",
        "                if dependency not in service_info[service][\"dependencies\"]:\n",
        "                    service_info[service][\"dependencies\"].append(dependency)\n",
        "                if dependency not in service_info:\n",
        "                    service_info[dependency] = {\"dependencies\": [], \"dependents\": []}\n",
        "                if service not in service_info[dependency][\"dependents\"]:\n",
        "                    service_info[dependency][\"dependents\"].append(service)\n",
        "\n",
        "    # print(\"Service info processed successfully.\")\n",
        "    # print(f\"Sample service info: {list(service_info.items())[:5]}\")\n",
        "\n",
        "    # Main function to initiate and manage the root cause analysis process\n",
        "    if __name__ == \"__main__\":\n",
        "        main()\n",
        "\n",
        "print(\"Results Saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "MwinFg1Mw6F1",
        "outputId": "20e55d7a-9867-41ea-e1e1-a7025cf6272d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n",
            "Executing Issue# 0\n",
            "Detected Top Service\n",
            "\n",
            "                   microservice           timestamp  availability_Average  \\\n",
            "35  PetSearch_AWS::ECS::Fargate 2023-04-18 22:32:00                 100.0   \n",
            "\n",
            "    latency_Average  latency_p50  latency_p90  latency_p95  latency_p99  \\\n",
            "35          0.00655     0.004664     0.011527     0.019289     0.037435   \n",
            "\n",
            "    requests_Sum  availability_Average_MAD  latency_Average_MAD  \\\n",
            "35         734.0                       0.0             0.184014   \n",
            "\n",
            "    latency_p50_MAD  latency_p90_MAD  latency_p95_MAD  latency_p99_MAD  \\\n",
            "35         0.000042         0.001036         0.007625         1.678968   \n",
            "\n",
            "    requests_Sum_MAD  Max_MAD_Score Metric_With_Max_MAD  \n",
            "35         50.408475      50.408475        requests_Sum  \n",
            "End of main function.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3b8e08b5c1c1>\u001b[0m in \u001b[0;36m<cell line: 455>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;31m# Main function to initiate and manage the root cause analysis process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results Saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-3b8e08b5c1c1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# print(row_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;31m# embedding = model_e.encode(row_text)  # Use for Synthtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-3b8e08b5c1c1>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move tensor to CPU before converting to numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1138\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 )\n\u001b[1;32m    689\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    691\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "opfgVG7vdKTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M8T8AgZtK4a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# import pandas as pd\n",
        "# import json\n",
        "# from scipy.stats import median_abs_deviation\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "# from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# # Load dataset from a specified CSV file\n",
        "# print(f\"Loading dataset from '{file_path_test}'...\")\n",
        "# metrics_data = pd.read_csv(file_path_test)\n",
        "# metrics_data['timestamp'] = pd.to_datetime(metrics_data['timestamp'])  # Convert 'timestamp' column to datetime objects\n",
        "# print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# # Load JSON data containing service information\n",
        "# print(f\"Loading JSON data from '{json_path}'...\")\n",
        "# with open(json_path, 'r') as f:\n",
        "#     data = json.load(f)\n",
        "#     print(\"JSON data loaded successfully.\")\n",
        "#     print(\"Sample JSON data:\", data[:5])\n",
        "\n",
        "# # Process the JSON data to build a dictionary mapping services to their dependencies and dependents\n",
        "# service_info = {}\n",
        "# for path in data:\n",
        "#     if len(path) > 1:\n",
        "#         for i in range(len(path) - 1):\n",
        "#             service = path[i]\n",
        "#             dependency = path[i + 1]\n",
        "#             if service not in service_info:\n",
        "#                 service_info[service] = {'dependencies': [], 'dependents': []}\n",
        "#             if dependency not in service_info[service]['dependencies']:\n",
        "#                 service_info[service]['dependencies'].append(dependency)\n",
        "#             if dependency not in service_info:\n",
        "#                 service_info[dependency] = {'dependencies': [], 'dependents': []}\n",
        "#             if service not in service_info[dependency]['dependents']:\n",
        "#                 service_info[dependency]['dependents'].append(service)\n",
        "\n",
        "# print(\"Service info processed successfully.\")\n",
        "# print(f\"Sample service info: {list(service_info.items())[:2]}\")\n",
        "\n",
        "# # Function to check if a service exists in the loaded service information\n",
        "# def service_exists(service_name):\n",
        "#     exists = service_name in service_info\n",
        "#     print(f\"Service '{service_name}' exists: {exists}\")\n",
        "#     return exists\n",
        "\n",
        "# # Function to calculate Median Absolute Deviation (MAD) scores for given metrics\n",
        "# def calculate_mad_scores(dataframe):\n",
        "#     print(\"Calculating MAD scores...\")\n",
        "#     metrics = ['availability_Average', 'latency_Average', 'latency_p50', 'latency_p90', 'latency_p95', 'latency_p99', 'requests_Sum']\n",
        "#     mad_columns = [m + '_MAD' for m in metrics]\n",
        "#     for m in metrics:\n",
        "#         dataframe.loc[:, m + '_MAD'] = dataframe.groupby('microservice')[m].transform(lambda x: median_abs_deviation(x, scale='normal'))\n",
        "#     dataframe['Max_MAD_Score'] = dataframe[mad_columns].max(axis=1)\n",
        "#     dataframe['Metric_With_Max_MAD'] = dataframe[mad_columns].idxmax(axis=1).str.replace('_MAD', '')\n",
        "#     print(\"MAD scores calculated successfully.\")\n",
        "#     return dataframe.loc[dataframe.groupby('microservice')['Max_MAD_Score'].idxmax()]\n",
        "\n",
        "# # Function to retrieve related anomalies based on service name and consider MAD scores for filtering\n",
        "# def retrieve_related_anomalies(service_name, target_mad_score, mad_tolerance=10, top_k=5):\n",
        "#     \"\"\"\n",
        "#     Retrieve related anomalies based on the service name, target MAD score within a tolerance range,\n",
        "#     and consider textual representations associated with the vectors.\n",
        "\n",
        "#     Parameters:\n",
        "#         service_name (str): Name of the service to query for.\n",
        "#         target_mad_score (float): Target MAD score to filter entries close to this score.\n",
        "#         top_k (int): Number of top results to retrieve.\n",
        "\n",
        "#     Returns:\n",
        "#         list: A list of textual representations or a message if no data is found.\n",
        "#     \"\"\"\n",
        "#     print(f\"Retrieving textual representations for service: {service_name} with consideration of MAD scores.\")\n",
        "#     query_text = service_name\n",
        "#     query_embedding = model_e.encode(query_text, convert_to_tensor=True).cpu().numpy().tolist()\n",
        "\n",
        "#     # Set up the metadata filter with MAD score range\n",
        "#     metadata_filter = {\n",
        "#         \"root_cause_node\": True,\n",
        "#         \"Max_MAD_Score\": {\"$gte\": target_mad_score - mad_tolerance, \"$lte\": target_mad_score + mad_tolerance}\n",
        "#     }\n",
        "\n",
        "#     try:\n",
        "#         # Perform the query with the metadata filter\n",
        "#         query_result = index.query(vector=query_embedding, top_k=top_k, include_metadata=True, filter=metadata_filter)\n",
        "#         print(f\"Retrieved {len(query_result['matches'])} matches.\")\n",
        "#         if not query_result['matches']:\n",
        "#             return [\"Previously, no historical logs have been logged for issues matching the current query parameters.\"]\n",
        "\n",
        "#         # Extract the textual representation from the embeddings or associated data fields\n",
        "#         return [match['metadata']['textual_representation'] for match in query_result['matches']]\n",
        "#     except Exception as e:\n",
        "#         print(\"Query failed with error:\", str(e))\n",
        "#         return [\"Previously, no historical posts were logged for issues matching the current query parameters.\"]\n",
        "\n",
        "\n",
        "\n",
        "# # Function to generate analysis prompts for anomalies detected in services\n",
        "# def generate_analysis_prompt(service_name, mad_score, affected_metric, dependencies, dependents, retrieval_results):\n",
        "#     dependencies_formatted = ', '.join(dependencies) if dependencies else 'None'\n",
        "#     dependents_formatted = ', '.join(dependents) if dependents else 'None'\n",
        "#     historical_anomalies = \"\\n\".join([f\"{idx + 1}. {anomaly}\" for idx, anomaly in enumerate(retrieval_results)])\n",
        "\n",
        "#     prompt = f\"\"\"\n",
        "#     An anomaly with a Median Absolute Deviation (MAD) score of {mad_score} has been detected in the '{service_name}' service's '{affected_metric}' metric. This service is a critical component of a pet adoption website's microservices architecture.\n",
        "#     **Dependencies involved**: {dependencies_formatted}\n",
        "#     **Dependents impacted**: {dependents_formatted}\n",
        "#     **Historical related anomalies**:\n",
        "#     {historical_anomalies}\n",
        "#     Your analysis should focus on identifying a singular root cause from among the dependencies that directly contributes to the anomaly in the '{affected_metric}' metric. Additionally, pinpoint the primary dependent (target node) that is most directly affected by this anomaly.\n",
        "#     Please provide a concise and focused hypothesis on:\n",
        "#     1. The singular root cause node among the dependencies.\n",
        "#     2. The primary target node among the dependents directly impacted by this anomaly.\n",
        "#     Your analysis will guide subsequent investigation and mitigation efforts.\"\"\"\n",
        "#     print(prompt)\n",
        "#     return prompt\n",
        "\n",
        "# def clean_hypothesis(hypothesis):\n",
        "#     \"\"\"Process the hypothesis to refine and extract the relevant section.\"\"\"\n",
        "#     lines = hypothesis.split('\\n')\n",
        "#     final_hypothesis_start = next((i for i, line in enumerate(lines) if \"Final Hypothesis:\" in line), None)\n",
        "#     cleaned_hypothesis = '\\n'.join(lines[final_hypothesis_start + 1:]) if final_hypothesis_start is not None else hypothesis\n",
        "#     print(f\"Cleaned Hypothesis: {cleaned_hypothesis}\")\n",
        "#     return cleaned_hypothesis\n",
        "\n",
        "# def analyze_root_cause(anomaly_row):\n",
        "#     \"\"\"Analyze root cause.\"\"\"\n",
        "#     service_name = anomaly_row['microservice']\n",
        "#     mad=anomaly_row['Max_MAD_Score']\n",
        "#     print(f\"Analyzing root cause for service: {service_name}\")\n",
        "#     if not service_exists(service_name):\n",
        "#         return \"Service information not found.\", \"\", []\n",
        "\n",
        "#     retrieval_results = retrieve_related_anomalies(service_name,mad)\n",
        "#     prompt = generate_analysis_prompt(\n",
        "#         service_name,\n",
        "#         anomaly_row['Max_MAD_Score'],\n",
        "#         anomaly_row['Metric_With_Max_MAD'],\n",
        "#         service_info[service_name]['dependencies'],\n",
        "#         service_info[service_name]['dependents'],\n",
        "#         retrieval_results\n",
        "#     )\n",
        "\n",
        "#     print(f\"Retrieving and generating response based on the prompt...\")\n",
        "#     response = pipe(prompt, max_new_tokens=1000, num_return_sequences=1, temperature=0.7, top_p=0.9)[0]['generated_text']\n",
        "#     print(f\"Response generated: {response}\")\n",
        "#     cleaned_response = clean_hypothesis(response)\n",
        "#     return prompt, cleaned_response,response\n",
        "\n",
        "# # Main function to initiate and manage the root cause analysis process\n",
        "# def main():\n",
        "#     try:\n",
        "#         print(\"Starting main function...\")\n",
        "#         metrics_data_filtered = calculate_mad_scores(metrics_data)\n",
        "#         top_services = metrics_data_filtered.sort_values(by='Max_MAD_Score', ascending=False).head(1)\n",
        "\n",
        "#         results = []\n",
        "\n",
        "#         for _, row in top_services.iterrows():\n",
        "#             print(f\"Analyzing root cause for service: {row['microservice']}\")\n",
        "#             if service_exists(row['microservice']):\n",
        "#                 prompt, cleaned_response,response = analyze_root_cause(row)\n",
        "#                 results.append({\n",
        "#                     'TestCase': testcase,\n",
        "#                     'Issue': issue,\n",
        "#                     'Service': row['microservice'],\n",
        "#                     'Timestamp': row['timestamp'].strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "#                     'MAD Score': row['Max_MAD_Score'],\n",
        "#                     'Affected Metric': row['Metric_With_Max_MAD'],\n",
        "#                     'Prompt': prompt,\n",
        "#                     'Response': response,\n",
        "#                     'Hypothesis': cleaned_response\n",
        "#                 })\n",
        "#                 print(f\"Analysis complete for service: {row['microservice']}\")\n",
        "#             else:\n",
        "#                 print(f\"Service {row['microservice']} not found in service information.\")\n",
        "#         results_df = pd.DataFrame(results)\n",
        "#         print(results_df)\n",
        "#         results_df.to_csv(f'/content/output_{testcase}_{issue}.csv')\n",
        "#         print(\"Analysis results saved.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "faf5b12dbb5f439eb17c08c05dde1dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f490a25d9c9845379ab661ebc3188c01",
              "IPY_MODEL_bd8213dc373f4cda8a7533fd619728f6",
              "IPY_MODEL_eb072299c00f47f9b1e9b2b72994ce94"
            ],
            "layout": "IPY_MODEL_ee67bb3f13db457eb3010c7cacff0b3f"
          }
        },
        "f490a25d9c9845379ab661ebc3188c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_301e2528247e4f82b38ad166c2fb424a",
            "placeholder": "​",
            "style": "IPY_MODEL_b68e18ab4a8f4416936bd9f42e7348c1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "bd8213dc373f4cda8a7533fd619728f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e58a6c0fab349678dcc8fbe8ab40e65",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10aed61ee49a4344882845f89614573f",
            "value": 366
          }
        },
        "eb072299c00f47f9b1e9b2b72994ce94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b4a49613f04546b7ab898c3be09131",
            "placeholder": "​",
            "style": "IPY_MODEL_f123efea6e664245a589a05c61bb28c5",
            "value": " 366/366 [00:00&lt;00:00, 21.7kB/s]"
          }
        },
        "ee67bb3f13db457eb3010c7cacff0b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301e2528247e4f82b38ad166c2fb424a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68e18ab4a8f4416936bd9f42e7348c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e58a6c0fab349678dcc8fbe8ab40e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10aed61ee49a4344882845f89614573f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74b4a49613f04546b7ab898c3be09131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f123efea6e664245a589a05c61bb28c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1609db3b0e24b368ec026342436f3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df9a9a14c2bf4b508c62698e1221e28b",
              "IPY_MODEL_c3710d2c055c47d2bbde81ec0e1fd428",
              "IPY_MODEL_8fea67e9c20b498da386db257975ce7f"
            ],
            "layout": "IPY_MODEL_91c40ecaf8ee4263bd5a956f2d44ffca"
          }
        },
        "df9a9a14c2bf4b508c62698e1221e28b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cea35f2221648a5959355851ecd43a8",
            "placeholder": "​",
            "style": "IPY_MODEL_a23e1a4a66504fc99d16dfa66cc86d42",
            "value": "vocab.txt: 100%"
          }
        },
        "c3710d2c055c47d2bbde81ec0e1fd428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1fb7ff0cd464212907ebda34b22bed9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd9f02224bc04fedb3be03b9988177c5",
            "value": 231508
          }
        },
        "8fea67e9c20b498da386db257975ce7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae43eabd30d402eb1d5782059bae459",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9915743ef949069a29bb101e9c5a32",
            "value": " 232k/232k [00:00&lt;00:00, 674kB/s]"
          }
        },
        "91c40ecaf8ee4263bd5a956f2d44ffca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cea35f2221648a5959355851ecd43a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a23e1a4a66504fc99d16dfa66cc86d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1fb7ff0cd464212907ebda34b22bed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd9f02224bc04fedb3be03b9988177c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bae43eabd30d402eb1d5782059bae459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9915743ef949069a29bb101e9c5a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4979407108be4c069701572a1b650175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80d1bbda3fdb4bf494ae5541f5838c8b",
              "IPY_MODEL_2046aa6f9aa24978a2d2c67c70f267dc",
              "IPY_MODEL_96b829b57af0410a8c73d106535476ca"
            ],
            "layout": "IPY_MODEL_7d04a1858e9342ef96042f323708ccae"
          }
        },
        "80d1bbda3fdb4bf494ae5541f5838c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8ad6b25e364a6bbb2b375413986fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_3e7da2089e824c47bebad18c385272b6",
            "value": "tokenizer.json: 100%"
          }
        },
        "2046aa6f9aa24978a2d2c67c70f267dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7af8fc633c7341cabc5dbfcdc9f169bd",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e4294223f7442849c54d1b84b6c5385",
            "value": 711396
          }
        },
        "96b829b57af0410a8c73d106535476ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d1f9a5ba09f43c682ad09b2ec479faa",
            "placeholder": "​",
            "style": "IPY_MODEL_94e0ec413d2845cc92567bbda3d7e877",
            "value": " 711k/711k [00:00&lt;00:00, 1.05MB/s]"
          }
        },
        "7d04a1858e9342ef96042f323708ccae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8ad6b25e364a6bbb2b375413986fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e7da2089e824c47bebad18c385272b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7af8fc633c7341cabc5dbfcdc9f169bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4294223f7442849c54d1b84b6c5385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d1f9a5ba09f43c682ad09b2ec479faa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94e0ec413d2845cc92567bbda3d7e877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a0364e5fa7040af96a6c7e6c3d6fd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_393ace59578c4a23bcd18fac0dc20b67",
              "IPY_MODEL_84571623e02f48cba2c32cf13b7c2f3e",
              "IPY_MODEL_820cf438e2534cb1a210c553e7f2a4f8"
            ],
            "layout": "IPY_MODEL_bfb41e046a2f49ad9db84e9b6ebe24c3"
          }
        },
        "393ace59578c4a23bcd18fac0dc20b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f84436f55b4599890b4b913273e2f5",
            "placeholder": "​",
            "style": "IPY_MODEL_803b3c8f7aa54f17be2dd4359f865462",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "84571623e02f48cba2c32cf13b7c2f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_238b07734d5046e5a9ac66fd57632dd8",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3170e94dbf84b929208d8241a7d6856",
            "value": 125
          }
        },
        "820cf438e2534cb1a210c553e7f2a4f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_774c09b3ff0c4fa48da168327848094e",
            "placeholder": "​",
            "style": "IPY_MODEL_da78b41e88334cbe9aa49cb1c36d5e39",
            "value": " 125/125 [00:00&lt;00:00, 4.25kB/s]"
          }
        },
        "bfb41e046a2f49ad9db84e9b6ebe24c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f84436f55b4599890b4b913273e2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "803b3c8f7aa54f17be2dd4359f865462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "238b07734d5046e5a9ac66fd57632dd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3170e94dbf84b929208d8241a7d6856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "774c09b3ff0c4fa48da168327848094e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da78b41e88334cbe9aa49cb1c36d5e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ea84cff156a472e990844f6c77cc5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d3db8f6f81741f09eb90db30a79d531",
              "IPY_MODEL_60f16ffa1f8740cbbb9c5f1b77004933",
              "IPY_MODEL_0a55e33d6f3b44dfa755b0ec7449dced"
            ],
            "layout": "IPY_MODEL_6694eb9a868249dd867fe80c62b248eb"
          }
        },
        "6d3db8f6f81741f09eb90db30a79d531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d42aad45aa7b4865ad1ff79ee38bef09",
            "placeholder": "​",
            "style": "IPY_MODEL_36760fb3c1594a3ab1e29b45a1817e3b",
            "value": "config.json: 100%"
          }
        },
        "60f16ffa1f8740cbbb9c5f1b77004933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18c48924849e4d2e8f4f1536b7d03a71",
            "max": 779,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61d7a308248749f0996f4027f2e37a42",
            "value": 779
          }
        },
        "0a55e33d6f3b44dfa755b0ec7449dced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56718a46578540d69ba6e113d55acbd3",
            "placeholder": "​",
            "style": "IPY_MODEL_2e12f942b5ea4498a0ea4ef7f76e867a",
            "value": " 779/779 [00:00&lt;00:00, 36.8kB/s]"
          }
        },
        "6694eb9a868249dd867fe80c62b248eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d42aad45aa7b4865ad1ff79ee38bef09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36760fb3c1594a3ab1e29b45a1817e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18c48924849e4d2e8f4f1536b7d03a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61d7a308248749f0996f4027f2e37a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56718a46578540d69ba6e113d55acbd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e12f942b5ea4498a0ea4ef7f76e867a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "298c74d429084086bb41bfa10915f4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f4fe3a8e809469a8923299602d894ed",
              "IPY_MODEL_e8efa9c183784075a885d3b870709a5a",
              "IPY_MODEL_04a6d077863b40188d36e6adb301aef1"
            ],
            "layout": "IPY_MODEL_2bb69366e9164e2088e25f01a6c37def"
          }
        },
        "7f4fe3a8e809469a8923299602d894ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba447f835ad3419cb814f85fdacb0889",
            "placeholder": "​",
            "style": "IPY_MODEL_ae5e4c4b657746f987f908342e0e34ee",
            "value": "model.safetensors: 100%"
          }
        },
        "e8efa9c183784075a885d3b870709a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c4b0050e70497789a6518821d325db",
            "max": 1340616616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a09054bfdee45599778958ed5fd6dba",
            "value": 1340616616
          }
        },
        "04a6d077863b40188d36e6adb301aef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90febd569aac481795bb38f53b199531",
            "placeholder": "​",
            "style": "IPY_MODEL_6638a7fc4be74410b0a202cf4413bfff",
            "value": " 1.34G/1.34G [01:00&lt;00:00, 22.5MB/s]"
          }
        },
        "2bb69366e9164e2088e25f01a6c37def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba447f835ad3419cb814f85fdacb0889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae5e4c4b657746f987f908342e0e34ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26c4b0050e70497789a6518821d325db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a09054bfdee45599778958ed5fd6dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90febd569aac481795bb38f53b199531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6638a7fc4be74410b0a202cf4413bfff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}